{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Classical Composers from Spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to accurately predict composers from spectrograms of their compositions. We're going to strip away everything about a composition other than its frequency distribution through time if the composition were played by a single instrument (organ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "I found a great data set on a subreddit for DJs who use MIDI files to sample classical music.\n",
    "\n",
    "It included 370,000 MIDI files, which I filtered down to classical compositions.\n",
    "\n",
    "Unfortunately, the data is a mess. Some folders are composers, some are compositions, and there's no clear order to any of the files.\n",
    "\n",
    "![messy data](./normalization_1.png)\n",
    "\n",
    "We need to get things in a roughly sensible order. I moved compositions to the folder of their composer, merged duplicate folders, and extracted any zipped midi files. This gives us a list of folders named by composer.\n",
    "\n",
    "We'll need to normalize our folders and files next.\n",
    "\n",
    "### Normalization\n",
    "Since we'll use folder names as our labels, let's normalize our folder names first.\n",
    "\n",
    "```py\n",
    "from pathlib import Path\n",
    "from sys import argv\n",
    "\n",
    "# Downcases the names of all directories inside the current directory.\n",
    "\n",
    "path = Path('.')\n",
    "dirs = [file for file in path.iterdir() if file.is_dir()]\n",
    "for (x, dir) in enumerate(dirs):\n",
    "    print('Changing', dir)\n",
    "    dir.rename(dir.name.lower())\n",
    "print('Done')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's normalize our file names to get them ready for conversion.\n",
    "\n",
    "```py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from pathlib import Path\n",
    "from sys import argv\n",
    "\n",
    "# Recursive Renaming!\n",
    "# Converts all files in a directory to the format [foldername]_[filenumber]\n",
    "# WARNING! By default, operates on root directory. Otherwise pass in a folder name\n",
    "# as first argv.\n",
    "def rename_files(folder_path, folder_name):\n",
    "    files = [file for file in folder_path.iterdir() if file.is_file() and (\"mid\" in file.name or \"MID\" in file.name)]\n",
    "    for (x, file) in enumerate(files):\n",
    "        print('Renaming', file)\n",
    "        file.rename(f'{folder_path}/{folder_name}_{x}.mid')\n",
    "    print('Done')\n",
    "\n",
    "def recursive_rename(dir_path):\n",
    "    folders = [folder for folder in dir_path.iterdir() if folder.is_dir()]\n",
    "    for folder in folders:\n",
    "        print('Renaming:', folder.name)\n",
    "        rename_files(root_path/f'{folder.name}', folder.name)\n",
    "\n",
    "root_path = argv[1] if len(argv) > 1 else Path('.')\n",
    "recursive_rename(root_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIDI to WAV\n",
    "Next we need to convert our MIDI files to WAV files, so we can process them into a spectrogram.\n",
    "\n",
    "MIDI files not only encode the notes of a given composition, but also data about the instruments. Our source data has symphonies, pieces for solo guitar, piano concertos, and just about everything in between. Some composers wrote heavily for certain instrument groups, so we *could* use this instrumentation data to predict.\n",
    "\n",
    "But for this project we'll see if we can find patterns in only the notes themselves. To do this, we need to convert all midis to the same synth. All compositions will use an organ-like sound.\n",
    "\n",
    "We'll use a [NodeJS package called synth](https://github.com/patrickroberts/synth-js) to handle the conversion.\n",
    "\n",
    "Here's a script that calls a companion bash script to convert all midi files in a directory. The bash and python scripts should be in the same directory. You can call it like this: ```./convert_all_midi_files_script.py ./my-compositions-directory-here```\n",
    "\n",
    "```py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from pathlib import Path\n",
    "from sys import argv\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "# Takes in a directory and converts all midi files to .wav\n",
    "\n",
    "def convert_all_midi_to_wav(folder_path):\n",
    "    files = [file for file in folder_path.iterdir() if file.is_file() and (\"mid\" in file.name)]\n",
    "    for (x, file) in enumerate(files):\n",
    "        without_file_extension = file.name[:-4]\n",
    "        resolved = Path(f'{folder_path}/{without_file_extension}')\n",
    "        # synth adds .mid to input file by default, so we remove it first\n",
    "        if (Path(f'{resolved}.wav').is_file() == False):\n",
    "            # Check if .wav exists, so we can safely re-run the file without re-converting\n",
    "            # any files that were already converted\n",
    "            print('Converting...', without_file_extension)\n",
    "            subprocess.call(shlex.split(f'./convert_midi_to_wav.sh {resolved}'))\n",
    "\n",
    "\n",
    "def do_recursively_on_folders(dir_path, action):\n",
    "    folders = [folder for folder in dir_path.iterdir() if folder.is_dir()]\n",
    "    for folder in folders:\n",
    "        action(folder)\n",
    "\n",
    "path = Path(argv[1])\n",
    "do_recursively_on_folders(path, convert_all_midi_to_wav)\n",
    "\n",
    "```\n",
    "\n",
    "Here's the companion bash script.\n",
    "\n",
    "```sh\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "synth -i $1\n",
    "```\n",
    "\n",
    "Conversion may take a long time! Don't worry if something fails or the script gets interrupted. You can call the script as many times as you need without worrying about overwriting or losing the files you've already converted.\n",
    "\n",
    "![converting to .wav files](./conversion.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAV to Spectrogram\n",
    "\n",
    "With our data organized by composer and converted to WAV files, it's time to make our spectrograms!\n",
    "\n",
    "A [spectrogram](https://en.wikipedia.org/wiki/Spectrogram) is a visual representation of sound and its properties. \n",
    "\n",
    "Spectrograms plot frequency on the y axis and time on the x axis. The intensity of colors suggests a higher density of a given frequency at a point in time.\n",
    "\n",
    "Here's another script to convert all our WAV files to spectrograms. This will likely also take a while!\n",
    "\n",
    "```py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from pathlib import Path\n",
    "from sys import argv\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_spectrograms(folder_path):\n",
    "    files = [file for file in folder_path.iterdir() if file.is_file() and (\"wav\" in file.name)]\n",
    "    for (x, file) in enumerate(files):\n",
    "        without_file_extension = file.name[:-4]\n",
    "        resolved = Path(f'{folder_path}/{without_file_extension}')\n",
    "        if (Path(f'{resolved}.png').is_file() == False):\n",
    "            print('making spectro for', resolved)\n",
    "            sample_rate, samples = wavfile.read(file)\n",
    "            frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)\n",
    "            plt.pcolormesh(np.log(spectrogram))\n",
    "            plt.axis('off')\n",
    "            plt.ylim(ymax=40)\n",
    "            # plt.show() # Uncomment if you want to check out the plot.\n",
    "            plt.savefig(f'{resolved}.png', transparent=True)\n",
    "            # saves the file as a .png with transparent background (no border)\n",
    "\n",
    "def do_recursively_on_folders(dir_path, action):\n",
    "    folders = [folder for folder in dir_path.iterdir() if folder.is_dir()]\n",
    "    for folder in folders:\n",
    "        action(folder)\n",
    "\n",
    "if (len(argv) < 2):\n",
    "    raise Exception('Please specify a path')\n",
    "path = Path(argv[1])\n",
    "do_recursively_on_folders(path, create_spectrograms)\n",
    "```\n",
    "\n",
    "Converting...\n",
    "![spectrogram conversion](./spectro-conversion.gif)\n",
    "\n",
    "Look at these beautiful spectrograms!\n",
    "\n",
    "Here's a Mozart, which looks pretty tame and orderly.\n",
    "![mozart spectrogram](./mozart_sample.png)\n",
    "\n",
    "And here's early-20th century composer [Charles Griffes](https://en.wikipedia.org/wiki/Charles_Tomlinson_Griffes), looking far more intensely [Modern](https://en.wikipedia.org/wiki/Modernism_(music)).\n",
    "![charles griffes spectrogram](./griffes_sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Data to GCP\n",
    "We need to get our data to our GPU-enabled computer. I'm using a compute instance on [GCP](https://cloud.google.com) to train our model. This step assumes you're using GCP too. If you're not, just ignore this step!\n",
    "\n",
    "Let's zip up all our files to make them smaller and easier to send.\n",
    "\n",
    "```sh\n",
    "# Recursively zips our 'data' directory into a tarball named 'composition.tar.gz', excluding the wav and mid files we made in previous steps.\n",
    "tar -czvf compositions.tar.gz ./data --exclude=*.wav --exclude=*.mid\n",
    "\n",
    "```\n",
    "Cool. The easiest way to get our data to GCP is with [scp](https://www.garron.me/en/articles/scp.html).\n",
    "\n",
    "Yep, you guessed it, here's another script to send your files off to Google Cloud.\n",
    "\n",
    "```sh\n",
    "#! /bin/bash\n",
    "gcloud compute scp --recurse ./your-data-path-here your-gcp-instance-name-here:~/\n",
    "```\n",
    "\n",
    "You may need to move the files from your root directory in GCP to another location depending on what your notebook needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Sweet. We're ready to load our data into a model and start training!\n",
    "\n",
    "We'll start with [Resnet 34](https://www.kaggle.com/pytorch/resnet34), a model pre-trained for image recognition.\n",
    "\n",
    "Let's load in our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/compositions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use fastai's DataBunch class to get validation and training sets from our data. We'll train on 70% of the data and validate on 30%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3b2fe3e724da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataBunch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_pct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_tfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \"\"\"\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ds_tfms are the transformations we apply to the images. also resizes all images so they're 224x224 since this is what resnet34 was trained on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# crops and zooms/centers a little\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/vision/data.py\u001b[0m in \u001b[0;36mfrom_folder\u001b[0;34m(cls, path, train, valid, test, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;34m\"Create from imagenet style dataset in `path` with `train`,`valid`,`test` subfolders.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageClassificationDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageClassificationDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         if test: datasets.append(ImageClassificationDataset.from_single_folder(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/vision/data.py\u001b[0m in \u001b[0;36mfrom_folder\u001b[0;34m(cls, folder, classes, valid_pct, check_ext)\u001b[0m\n\u001b[1;32m     98\u001b[0m                    )->Union['ImageClassificationDataset', List['ImageClassificationDataset']]:\n\u001b[1;32m     99\u001b[0m         \u001b[0;34m\"Dataset of `classes` labeled images in `folder`. Optional `valid_pct` split validation set.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mfns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/core.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mFilePathList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;34m\"List of label subdirectories in imagenet-style `folder`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     classes = [d for d in folder.iterdir()\n\u001b[0m\u001b[1;32m     64\u001b[0m                if d.is_dir() and not d.name.startswith('.')]\n\u001b[1;32m     65\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/core.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mFilePathList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;34m\"List of label subdirectories in imagenet-style `folder`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     classes = [d for d in folder.iterdir()\n\u001b[0m\u001b[1;32m     64\u001b[0m                if d.is_dir() and not d.name.startswith('.')]\n\u001b[1;32m     65\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36miterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                 \u001b[0;31m# Yielding a path object for these makes little sense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(pathobj, *args)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/train'"
     ]
    }
   ],
   "source": [
    "data = ImageDataBunch.from_folder(path, valid_pct=0.3, ds_tfms=get_transforms(), size=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.from_folder` automatically looks through a directory of folders sorted by class and splits the files contained in each class into a `train` (training) and `valid` (validation) set.\n",
    "\n",
    "ImageDataBunch takes a few additional parameters to help prepare our data for our model. `get_transforms()` creates a list of transforms that we'll apply to our images using the ds_tfms parameter. These include transforms like cropping and zooming the images.\n",
    "\n",
    "We'll also set size to 224 since this 224x224 are the dimensions of the images used to train Resnet34."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check!\n",
    "Before we continue let's sanity check our data.\n",
    "\n",
    "First, let's peek a few rows to make sure they look right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PosixPath' object has no attribute 'show_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3d4a272cee03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's peek at a few rows, to make sure they look right.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute 'show_batch'"
     ]
    }
   ],
   "source": [
    "data.show_batch(rows=5, figsize=(7,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good. Let's make sure our class labels look alright:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PosixPath' object has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-27b44d915428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute 'classes'"
     ]
    }
   ],
   "source": [
    "print(data.classes)\n",
    "len(data.classes),data.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Let's go ahead and create our learner and train it with 5 cycles through the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PosixPath' object has no attribute 'c'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a0067ee47ddb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet34\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/vision/learner.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, arch, cut, pretrained, lin_ftrs, ps, custom_head, split_on, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cut'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mnf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_head\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcreate_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlin_ftrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute 'c'"
     ]
    }
   ],
   "source": [
    "learn = ConvLearner(data, models.resnet34, metrics=error_rate)\n",
    "learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model so we can use it in future sessions without having to wait for training again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4bc855441ef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'learn' is not defined"
     ]
    }
   ],
   "source": [
    "learn.save('res-34')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation and Analysis\n",
    "\n",
    "Let's see how well we're doing. Specifically, we want to understand where our model is confused. If the model's confusion seems reasonable, we're doing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll plot our `top_losses`. These are the images that the model was most confused about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(10, figsize=(15,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll plot a confusion matrix. A confusion matrix visualizes the labels that the model is most confused about. If you look to the above and below the sharp diagonal line, you'll see how many times a pair of labels was miscategorized.\n",
    "\n",
    "As you can see...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-d745160313a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minterp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'interp' is not defined"
     ]
    }
   ],
   "source": [
    "interp.plot_confusion_matrix(figsize=(12,12), dpi=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out our most confused pairs to see what we can learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.most_confused(min_val=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the Model with Resnet 50\n",
    "Let's take another pass at this problem using Resnet 50, a larger and deeper pre-trained model for image classification.\n",
    "\n",
    "We'll follow the same process as last time, just with slightly different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-80093727cf4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataBunch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_tfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# WHAT ARE THESE PARAMS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagenet_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path_img' is not defined"
     ]
    }
   ],
   "source": [
    "data = ImageDataBunch.from_folder(path_img, fnames, pat, ds_tfms=get_transforms(), size=299, bs=48)\n",
    "# WHAT ARE THESE PARAMS\n",
    "data.normalize(imagenet_stats)\n",
    "\n",
    "# Make our Learner with resnet50\n",
    "learn = ConvLearner(data, models.resnet50, metrics=error_rate)\n",
    "learn.fit_one_cycle(5)\n",
    "learn.save('res-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like things have improved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
